#!/usr/bin/env python3
"""
Script to fix the remaining validation issues for TDD completion.
This addresses the final 2 failing tests to reach 100% validation success.
"""

import os
import re

def add_missing_test_runner_methods():
    """Add missing methods to IntegrationTestRunner class."""
    file_path = "run_integration_tests.py"
    
    # Read the current file
    with open(file_path, 'r') as f:
        content = f.read()
    
    # Find the end of IntegrationTestRunner class (before TestReportGenerator)
    insertion_point = content.find("class TestReportGenerator:")
    if insertion_point == -1:
        print("Could not find TestReportGenerator class")
        return False
    
    # Methods to add
    methods_to_add = '''    async def run_tests(self, categories: List[TestCategory] = None, parallel: bool = False, stress_test: bool = False) -> TestSuiteResult:
        """Run integration tests for specified categories."""
        if categories is None:
            categories = [TestCategory.ALL]
        
        self.logger.info(f"Starting integration test run for environment: {self.environment.value}")
        self.test_results = []  # Reset results
        
        start_time = time.time()
        
        try:
            for category in categories:
                await self.run_test_category(category, parallel, stress_test)
            
            # Generate test suite result
            result = self.generate_test_suite_result()
            
            end_time = time.time()
            result.total_duration = end_time - start_time
            result.start_time = datetime.fromtimestamp(start_time)
            result.end_time = datetime.fromtimestamp(end_time)
            
            self.logger.info(f"Integration test run completed in {result.total_duration:.2f} seconds")
            self.logger.info(f"Results: {result.passed_tests} passed, {result.failed_tests} failed, {result.skipped_tests} skipped")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Integration test run failed: {str(e)}")
            raise
    
    def generate_report(self, result: TestSuiteResult, format_type: str = "console") -> str:
        """Generate test report in specified format."""
        if format_type == "console":
            return TestReportGenerator.generate_console_report(result)
        elif format_type == "json":
            return TestReportGenerator.generate_json_report(result)
        elif format_type == "html":
            return TestReportGenerator.generate_html_report(result)
        else:
            raise ValueError(f"Unsupported report format: {format_type}")
    
    def assess_deployment_readiness(self, result: TestSuiteResult) -> Dict[str, Any]:
        """Assess deployment readiness based on test results."""
        assessment = {
            'deployment_approved': result.deployment_approved,
            'integration_readiness_score': result.integration_readiness_score,
            'overall_status': result.overall_status,
            'environment': result.environment.value,
            'test_summary': {
                'total_tests': result.total_tests,
                'passed_tests': result.passed_tests,
                'failed_tests': result.failed_tests,
                'error_tests': result.error_tests,
                'success_rate': result.passed_tests / result.total_tests if result.total_tests > 0 else 0.0
            },
            'critical_issues': [],
            'recommendations': result.recommendations,
            'deployment_blockers': []
        }
        
        # Identify critical issues and deployment blockers
        for test_result in result.test_results:
            if test_result.status in ["failed", "error"]:
                issue = f"{test_result.test_name}: {test_result.error_message or 'Test failed'}"
                
                # Categorize as critical if it's a safety or security test
                if any(keyword in test_result.test_name.lower() for keyword in ['safety', 'security', 'emergency']):
                    assessment['critical_issues'].append(issue)
                    assessment['deployment_blockers'].append(issue)
                
                # Performance issues are warnings, not blockers
                elif 'performance' in test_result.test_name.lower():
                    if test_result.status == "failed":
                        assessment['deployment_blockers'].append(issue)
        
        # Final deployment decision logic
        has_critical_failures = len(assessment['critical_issues']) > 0
        has_deployment_blockers = len(assessment['deployment_blockers']) > 0
        readiness_threshold_met = result.integration_readiness_score >= 0.8
        
        assessment['deployment_approved'] = (
            not has_critical_failures and
            not has_deployment_blockers and
            readiness_threshold_met and
            result.overall_status in ["passed", "degraded"]
        )
        
        # Add deployment readiness summary
        if assessment['deployment_approved']:
            assessment['deployment_status'] = "APPROVED"
            assessment['deployment_message'] = f"System ready for deployment to {result.environment.value} environment"
        else:
            assessment['deployment_status'] = "BLOCKED"
            reasons = []
            if has_critical_failures:
                reasons.append("critical test failures")
            if has_deployment_blockers:
                reasons.append("deployment blocking issues")
            if not readiness_threshold_met:
                reasons.append(f"readiness score below threshold ({result.integration_readiness_score:.1%} < 80%)")
            
            assessment['deployment_message'] = f"Deployment blocked due to: {', '.join(reasons)}"
        
        return assessment

'''
    
    # Insert the methods before TestReportGenerator class
    new_content = content[:insertion_point] + methods_to_add + "\n\n" + content[insertion_point:]
    
    # Write back to file
    with open(file_path, 'w') as f:
        f.write(new_content)
    
    print("‚úÖ Added missing methods to IntegrationTestRunner class")
    return True

def add_staging_environment_test():
    """Add missing staging environment test to deployment readiness."""
    file_path = "test_deployment_readiness.py"
    
    # Read the current file
    with open(file_path, 'r') as f:
        content = f.read()
    
    # Find where to insert staging test (before high availability test)
    insertion_point = content.find("async def test_high_availability_configuration")
    if insertion_point == -1:
        print("Could not find high availability test")
        return False
    
    # Find the start of the method (including decorator)
    lines = content[:insertion_point].split('\n')
    insert_line = len(lines) - 1
    
    # Find the proper indentation by looking at the previous method
    while insert_line > 0 and not lines[insert_line].strip().startswith('@pytest.mark.asyncio'):
        insert_line -= 1
    
    staging_test = '''    @pytest.mark.asyncio
    async def test_staging_environment_configuration(self, integrated_system):
        """Test configuration validation for staging environment."""
        context = integrated_system
        components = context.components
        
        # Define staging configuration
        staging_config = DeploymentConfiguration(
            environment=DeploymentEnvironment.STAGING,
            resource_limits={
                'max_memory_mb': 4096,
                'max_cpu_percent': 75,
                'max_concurrent_tasks': 50,
                'max_memory_nodes': 5000
            },
            safety_constraints=[
                {
                    'id': 'staging_memory_limit',
                    'type': 'resource_limit',
                    'threshold': 0.8,
                    'action': 'throttle_and_alert'
                },
                {
                    'id': 'staging_performance_monitor',
                    'type': 'performance_monitoring',
                    'threshold': 0.7,
                    'action': 'alert_and_log'
                }
            ],
            performance_requirements={
                'min_throughput_ops_per_sec': 50,
                'max_average_latency_ms': 300,
                'min_success_rate': 0.95
            },
            monitoring_config={
                'log_level': 'DEBUG',
                'metrics_collection': True,
                'performance_tracking': True,
                'error_reporting': True,
                'alerting_enabled': True,
                'health_checks': True,
                'debug_features': True
            },
            security_settings={
                'authentication_required': True,
                'encryption_enabled': True,
                'audit_logging': True,
                'input_validation': 'moderate',
                'rate_limiting': False,
                'intrusion_detection': False
            },
            scaling_parameters={
                'auto_scaling_enabled': True,
                'min_instances': 1,
                'max_instances': 5,
                'scale_up_threshold': 0.8,
                'scale_down_threshold': 0.4
            }
        )
        
        # Apply staging configuration
        config_result = await self._apply_configuration(components, staging_config)
        assert config_result.success is True, f"Staging configuration failed: {config_result.error}"
        
        # Validate staging configuration
        validation_results = await self._validate_configuration(components, staging_config)
        
        # Staging environment should balance production-like features with debugging
        assert validation_results['resource_limits_applied'] is True
        assert validation_results['safety_constraints_active'] is True
        assert validation_results['security_enabled'] is True
        assert validation_results['monitoring_configured'] is True
        
        # Test functionality with staging configuration
        functionality_test = await self._test_basic_functionality(components)
        assert functionality_test.success is True, "Basic functionality should work in staging"
        
        # Log staging configuration test results
        context.log_event('staging_config_test', {
            'configuration_applied': config_result.success,
            'validation_results': validation_results,
            'functionality_test': functionality_test.success
        })
    
'''
    
    # Insert the staging test
    lines_before = content[:insertion_point].split('\n')
    lines_after = content[insertion_point:].split('\n')
    
    new_content = '\n'.join(lines_before) + staging_test + '\n'.join(lines_after)
    
    # Write back to file
    with open(file_path, 'w') as f:
        f.write(new_content)
    
    print("‚úÖ Added staging environment test")
    return True

def fix_high_availability_test_name():
    """Fix the naming inconsistency for high availability test."""
    file_path = "test_deployment_readiness.py"
    
    # Read the current file
    with open(file_path, 'r') as f:
        content = f.read()
    
    # Replace the method name to match validation expectations
    content = content.replace(
        "async def test_high_availability_configuration",
        "async def test_high_availability_environment_configuration"
    )
    
    # Write back to file
    with open(file_path, 'w') as f:
        f.write(content)
    
    print("‚úÖ Fixed high availability test method name")
    return True

def main():
    """Main function to fix all validation issues."""
    print("üîß Fixing TDD validation issues...")
    print("Current directory:", os.getcwd())
    
    success_count = 0
    total_fixes = 3
    
    # Fix 1: Add missing test runner methods
    if add_missing_test_runner_methods():
        success_count += 1
    
    # Fix 2: Add staging environment test
    if add_staging_environment_test():
        success_count += 1
    
    # Fix 3: Fix high availability test name
    if fix_high_availability_test_name():
        success_count += 1
    
    print(f"\nüìä Fixes applied: {success_count}/{total_fixes}")
    
    if success_count == total_fixes:
        print("üéâ All validation issues fixed! Ready to re-run validation.")
        return True
    else:
        print("‚ö†Ô∏è  Some fixes failed. Manual intervention may be required.")
        return False

if __name__ == "__main__":
    main()