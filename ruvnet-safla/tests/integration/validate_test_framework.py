#!/usr/bin/env python3
"""
SAFLA Integration Test Framework Validation Script

This script validates the integration test framework by running a comprehensive
suite of validation tests to ensure all components are properly configured
and the testing infrastructure is working correctly.

Following TDD principles:
1. Red: Validate that tests fail when they should
2. Green: Validate that tests pass when they should  
3. Refactor: Validate that the test framework is maintainable and extensible
"""

import asyncio
import sys
import time
import traceback
from pathlib import Path
from typing import Dict, List, Any, Optional
import json
import logging
from dataclasses import dataclass, asdict
from enum import Enum

# Add the project root to the Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

class ValidationStatus(Enum):
    """Validation test status enumeration."""
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"

@dataclass
class ValidationResult:
    """Result of a validation test."""
    test_name: str
    status: ValidationStatus
    duration: float
    message: str
    details: Optional[Dict[str, Any]] = None
    error_traceback: Optional[str] = None

class TestFrameworkValidator:
    """
    Validates the SAFLA integration test framework.
    
    This validator ensures that:
    1. All test files are properly structured
    2. Test fixtures are correctly configured
    3. Mock components work as expected
    4. Performance monitoring is functional
    5. Error injection mechanisms work
    6. Test reporting is accurate
    """
    
    def __init__(self):
        self.results: List[ValidationResult] = []
        self.logger = self._setup_logging()
        
    def _setup_logging(self) -> logging.Logger:
        """Set up logging for validation."""
        logger = logging.getLogger("test_framework_validator")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    async def run_validation(self) -> Dict[str, Any]:
        """
        Run comprehensive validation of the test framework.
        
        Returns:
            Dict containing validation results and summary
        """
        self.logger.info("Starting SAFLA Integration Test Framework Validation")
        start_time = time.time()
        
        # Define validation tests
        validation_tests = [
            self._validate_test_file_structure,
            self._validate_test_fixtures,
            self._validate_mock_components,
            self._validate_performance_monitoring,
            self._validate_error_injection,
            self._validate_test_data_generation,
            self._validate_integration_context,
            self._validate_safety_constraints,
            self._validate_deployment_configurations,
            self._validate_test_runner_functionality,
            self._validate_reporting_mechanisms,
            self._validate_tdd_compliance
        ]
        
        # Run validation tests
        for test_func in validation_tests:
            try:
                await self._run_validation_test(test_func)
            except Exception as e:
                self.logger.error(f"Validation test {test_func.__name__} failed with error: {e}")
                self.results.append(ValidationResult(
                    test_name=test_func.__name__,
                    status=ValidationStatus.ERROR,
                    duration=0.0,
                    message=f"Validation test failed with error: {str(e)}",
                    error_traceback=traceback.format_exc()
                ))
        
        total_duration = time.time() - start_time
        
        # Generate validation summary
        summary = self._generate_validation_summary(total_duration)
        
        self.logger.info(f"Validation completed in {total_duration:.2f} seconds")
        return summary
    
    async def _run_validation_test(self, test_func) -> None:
        """Run a single validation test with timing and error handling."""
        test_name = test_func.__name__
        self.logger.info(f"Running validation test: {test_name}")
        
        start_time = time.time()
        try:
            result = await test_func()
            duration = time.time() - start_time
            
            if result is True:
                status = ValidationStatus.PASSED
                message = "Validation test passed successfully"
            elif result is False:
                status = ValidationStatus.FAILED
                message = "Validation test failed"
            elif isinstance(result, dict):
                status = ValidationStatus.PASSED if result.get('success', False) else ValidationStatus.FAILED
                message = result.get('message', 'No message provided')
            else:
                status = ValidationStatus.SKIPPED
                message = "Validation test was skipped"
            
            self.results.append(ValidationResult(
                test_name=test_name,
                status=status,
                duration=duration,
                message=message,
                details=result if isinstance(result, dict) else None
            ))
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"Validation test {test_name} failed: {e}")
            self.results.append(ValidationResult(
                test_name=test_name,
                status=ValidationStatus.ERROR,
                duration=duration,
                message=f"Test failed with exception: {str(e)}",
                error_traceback=traceback.format_exc()
            ))
    
    async def _validate_test_file_structure(self) -> Dict[str, Any]:
        """Validate that all required test files exist and are properly structured."""
        required_files = [
            "tests/integration/test_system_integration.py",
            "tests/integration/test_end_to_end_workflows.py", 
            "tests/integration/test_performance_integration.py",
            "tests/integration/test_safety_integration.py",
            "tests/integration/test_deployment_readiness.py",
            "tests/integration/conftest.py",
            "tests/integration/run_integration_tests.py"
        ]
        
        missing_files = []
        invalid_files = []
        
        for file_path in required_files:
            full_path = project_root / file_path
            if not full_path.exists():
                missing_files.append(file_path)
                continue
                
            # Basic syntax validation
            try:
                with open(full_path, 'r') as f:
                    content = f.read()
                    compile(content, str(full_path), 'exec')
            except SyntaxError as e:
                invalid_files.append(f"{file_path}: {e}")
        
        success = len(missing_files) == 0 and len(invalid_files) == 0
        
        return {
            'success': success,
            'message': f"File structure validation {'passed' if success else 'failed'}",
            'missing_files': missing_files,
            'invalid_files': invalid_files,
            'total_files_checked': len(required_files)
        }
    
    async def _validate_test_fixtures(self) -> Dict[str, Any]:
        """Validate that test fixtures are properly configured."""
        try:
            # Import conftest to check fixture definitions
            sys.path.insert(0, str(project_root / "tests" / "integration"))
            
            # Check for required fixtures
            required_fixtures = [
                'integration_context',
                'integrated_system', 
                'mock_external_services',
                'performance_monitor',
                'error_injector',
                'test_data_generator'
            ]
            
            # This is a basic check - in a real scenario we'd import and inspect
            conftest_path = project_root / "tests" / "integration" / "conftest.py"
            with open(conftest_path, 'r') as f:
                conftest_content = f.read()
            
            missing_fixtures = []
            for fixture in required_fixtures:
                if f"def {fixture}" not in conftest_content and f"async def {fixture}" not in conftest_content:
                    missing_fixtures.append(fixture)
            
            success = len(missing_fixtures) == 0
            
            return {
                'success': success,
                'message': f"Fixture validation {'passed' if success else 'failed'}",
                'missing_fixtures': missing_fixtures,
                'total_fixtures_checked': len(required_fixtures)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Fixture validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_mock_components(self) -> Dict[str, Any]:
        """Validate that mock components are properly implemented."""
        try:
            # Check for mock component classes in conftest
            conftest_path = project_root / "tests" / "integration" / "conftest.py"
            with open(conftest_path, 'r') as f:
                conftest_content = f.read()
            
            required_mocks = [
                'MockDeltaEvaluator',
                'MockMetaCognitiveEngine',
                'MockHybridMemory',
                'MockMCPOrchestrator',
                'MockSafetyValidator',
                'MockMemoryOptimizer'
            ]
            
            missing_mocks = []
            for mock in required_mocks:
                if f"class {mock}" not in conftest_content:
                    missing_mocks.append(mock)
            
            success = len(missing_mocks) == 0
            
            return {
                'success': success,
                'message': f"Mock component validation {'passed' if success else 'failed'}",
                'missing_mocks': missing_mocks,
                'total_mocks_checked': len(required_mocks)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Mock component validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_performance_monitoring(self) -> Dict[str, Any]:
        """Validate that performance monitoring is functional."""
        try:
            # Check for PerformanceMonitor class
            conftest_path = project_root / "tests" / "integration" / "conftest.py"
            with open(conftest_path, 'r') as f:
                conftest_content = f.read()
            
            required_methods = [
                'start_monitoring',
                'stop_monitoring', 
                'get_metrics',
                'generate_performance_report'
            ]
            
            missing_methods = []
            for method in required_methods:
                if f"def {method}" not in conftest_content and f"async def {method}" not in conftest_content:
                    missing_methods.append(method)
            
            has_performance_monitor = "class PerformanceMonitor" in conftest_content
            success = has_performance_monitor and len(missing_methods) == 0
            
            return {
                'success': success,
                'message': f"Performance monitoring validation {'passed' if success else 'failed'}",
                'has_performance_monitor': has_performance_monitor,
                'missing_methods': missing_methods,
                'total_methods_checked': len(required_methods)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Performance monitoring validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_error_injection(self) -> Dict[str, Any]:
        """Validate that error injection mechanisms are functional."""
        try:
            # Check for ErrorInjector class
            conftest_path = project_root / "tests" / "integration" / "conftest.py"
            with open(conftest_path, 'r') as f:
                conftest_content = f.read()
            
            required_methods = [
                'inject_component_failure',
                'inject_network_failure',
                'inject_memory_pressure',
                'inject_cpu_overload'
            ]
            
            missing_methods = []
            for method in required_methods:
                if f"def {method}" not in conftest_content and f"async def {method}" not in conftest_content:
                    missing_methods.append(method)
            
            has_error_injector = "class ErrorInjector" in conftest_content
            success = has_error_injector and len(missing_methods) == 0
            
            return {
                'success': success,
                'message': f"Error injection validation {'passed' if success else 'failed'}",
                'has_error_injector': has_error_injector,
                'missing_methods': missing_methods,
                'total_methods_checked': len(required_methods)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Error injection validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_test_data_generation(self) -> Dict[str, Any]:
        """Validate that test data generation is functional."""
        try:
            # Check for TestDataGenerator class
            conftest_path = project_root / "tests" / "integration" / "conftest.py"
            with open(conftest_path, 'r') as f:
                conftest_content = f.read()
            
            required_methods = [
                'generate_performance_data',
                'generate_memory_data',
                'generate_task_data',
                'generate_safety_test_data'
            ]
            
            missing_methods = []
            for method in required_methods:
                if f"def {method}" not in conftest_content and f"async def {method}" not in conftest_content:
                    missing_methods.append(method)
            
            has_test_data_generator = "class TestDataGenerator" in conftest_content
            success = has_test_data_generator and len(missing_methods) == 0
            
            return {
                'success': success,
                'message': f"Test data generation validation {'passed' if success else 'failed'}",
                'has_test_data_generator': has_test_data_generator,
                'missing_methods': missing_methods,
                'total_methods_checked': len(required_methods)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Test data generation validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_integration_context(self) -> Dict[str, Any]:
        """Validate that integration context management is functional."""
        try:
            # Check for IntegrationTestContext class
            conftest_path = project_root / "tests" / "integration" / "conftest.py"
            with open(conftest_path, 'r') as f:
                conftest_content = f.read()
            
            required_methods = [
                'setup_components',
                'cleanup_components',
                'get_component',
                'wait_for_system_stability'
            ]
            
            missing_methods = []
            for method in required_methods:
                if f"def {method}" not in conftest_content and f"async def {method}" not in conftest_content:
                    missing_methods.append(method)
            
            has_integration_context = "class IntegrationTestContext" in conftest_content
            success = has_integration_context and len(missing_methods) == 0
            
            return {
                'success': success,
                'message': f"Integration context validation {'passed' if success else 'failed'}",
                'has_integration_context': has_integration_context,
                'missing_methods': missing_methods,
                'total_methods_checked': len(required_methods)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Integration context validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_safety_constraints(self) -> Dict[str, Any]:
        """Validate that safety constraint testing is comprehensive."""
        try:
            # Check safety integration test file
            safety_test_path = project_root / "tests" / "integration" / "test_safety_integration.py"
            with open(safety_test_path, 'r') as f:
                safety_content = f.read()
            
            required_safety_tests = [
                'test_memory_safety_constraints',
                'test_delta_evaluation_safety_constraints',
                'test_mcp_orchestration_safety_constraints',
                'test_system_wide_emergency_stop',
                'test_component_specific_emergency_stop',
                'test_hierarchical_constraint_enforcement'
            ]
            
            missing_tests = []
            for test in required_safety_tests:
                if f"def {test}" not in safety_content and f"async def {test}" not in safety_content:
                    missing_tests.append(test)
            
            success = len(missing_tests) == 0
            
            return {
                'success': success,
                'message': f"Safety constraint validation {'passed' if success else 'failed'}",
                'missing_tests': missing_tests,
                'total_tests_checked': len(required_safety_tests)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Safety constraint validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_deployment_configurations(self) -> Dict[str, Any]:
        """Validate that deployment configuration testing is comprehensive."""
        try:
            # Check deployment readiness test file
            deployment_test_path = project_root / "tests" / "integration" / "test_deployment_readiness.py"
            with open(deployment_test_path, 'r') as f:
                deployment_content = f.read()
            
            required_environments = ['dev', 'staging', 'prod', 'ha']
            required_config_tests = [
                'test_development_environment_configuration',
                'test_staging_environment_configuration', 
                'test_production_environment_configuration',
                'test_high_availability_environment_configuration'
            ]
            
            missing_tests = []
            for test in required_config_tests:
                if f"def {test}" not in deployment_content and f"async def {test}" not in deployment_content:
                    missing_tests.append(test)
            
            success = len(missing_tests) == 0
            
            return {
                'success': success,
                'message': f"Deployment configuration validation {'passed' if success else 'failed'}",
                'missing_tests': missing_tests,
                'total_tests_checked': len(required_config_tests),
                'environments_supported': required_environments
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Deployment configuration validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_test_runner_functionality(self) -> Dict[str, Any]:
        """Validate that the test runner is properly implemented."""
        try:
            # Check test runner file
            runner_path = project_root / "tests" / "integration" / "run_integration_tests.py"
            with open(runner_path, 'r') as f:
                runner_content = f.read()
            
            required_classes = [
                'IntegrationTestRunner',
                'TestReportGenerator'
            ]
            
            required_methods = [
                'run_tests',
                'generate_report',
                'assess_deployment_readiness'
            ]
            
            missing_classes = []
            missing_methods = []
            
            for cls in required_classes:
                if f"class {cls}" not in runner_content:
                    missing_classes.append(cls)
            
            for method in required_methods:
                if f"def {method}" not in runner_content and f"async def {method}" not in runner_content:
                    missing_methods.append(method)
            
            success = len(missing_classes) == 0 and len(missing_methods) == 0
            
            return {
                'success': success,
                'message': f"Test runner validation {'passed' if success else 'failed'}",
                'missing_classes': missing_classes,
                'missing_methods': missing_methods,
                'total_classes_checked': len(required_classes),
                'total_methods_checked': len(required_methods)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Test runner validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_reporting_mechanisms(self) -> Dict[str, Any]:
        """Validate that reporting mechanisms are comprehensive."""
        try:
            # Check test runner for reporting functionality
            runner_path = project_root / "tests" / "integration" / "run_integration_tests.py"
            with open(runner_path, 'r') as f:
                runner_content = f.read()
            
            required_report_formats = ['console', 'json', 'html']
            required_report_methods = [
                'generate_console_report',
                'generate_json_report', 
                'generate_html_report'
            ]
            
            missing_methods = []
            for method in required_report_methods:
                if f"def {method}" not in runner_content and f"async def {method}" not in runner_content:
                    missing_methods.append(method)
            
            # Check for report format support
            format_support = {}
            for fmt in required_report_formats:
                format_support[fmt] = fmt in runner_content
            
            success = len(missing_methods) == 0 and all(format_support.values())
            
            return {
                'success': success,
                'message': f"Reporting mechanism validation {'passed' if success else 'failed'}",
                'missing_methods': missing_methods,
                'format_support': format_support,
                'total_methods_checked': len(required_report_methods)
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"Reporting mechanism validation failed: {str(e)}",
                'error': str(e)
            }
    
    async def _validate_tdd_compliance(self) -> Dict[str, Any]:
        """Validate that the test framework follows TDD principles."""
        try:
            test_files = [
                "tests/integration/test_system_integration.py",
                "tests/integration/test_end_to_end_workflows.py",
                "tests/integration/test_performance_integration.py",
                "tests/integration/test_safety_integration.py",
                "tests/integration/test_deployment_readiness.py"
            ]
            
            tdd_compliance_score = 0
            total_checks = 0
            compliance_details = {}
            
            for test_file in test_files:
                file_path = project_root / test_file
                if not file_path.exists():
                    continue
                    
                with open(file_path, 'r') as f:
                    content = f.read()
                
                file_compliance = {}
                
                # Check for proper test structure (Arrange-Act-Assert)
                test_functions = [line for line in content.split('\n') if line.strip().startswith('def test_') or line.strip().startswith('async def test_')]
                file_compliance['test_count'] = len(test_functions)
                
                # Check for docstrings (documentation)
                docstring_count = content.count('"""')
                file_compliance['has_docstrings'] = docstring_count > 0
                
                # Check for assertions
                assertion_count = content.count('assert')
                file_compliance['has_assertions'] = assertion_count > 0
                
                # Check for setup/teardown patterns
                has_setup = 'setup' in content.lower() or 'arrange' in content.lower()
                has_teardown = 'teardown' in content.lower() or 'cleanup' in content.lower()
                file_compliance['has_setup_teardown'] = has_setup and has_teardown
                
                # Check for error handling
                has_error_handling = 'try:' in content or 'except' in content or 'pytest.raises' in content
                file_compliance['has_error_handling'] = has_error_handling
                
                # Calculate compliance score for this file
                file_score = sum([
                    file_compliance['test_count'] > 0,
                    file_compliance['has_docstrings'],
                    file_compliance['has_assertions'],
                    file_compliance['has_setup_teardown'],
                    file_compliance['has_error_handling']
                ])
                
                tdd_compliance_score += file_score
                total_checks += 5
                compliance_details[test_file] = file_compliance
            
            overall_compliance = (tdd_compliance_score / total_checks) * 100 if total_checks > 0 else 0
            success = overall_compliance >= 80  # 80% compliance threshold
            
            return {
                'success': success,
                'message': f"TDD compliance validation {'passed' if success else 'failed'}",
                'overall_compliance_percentage': overall_compliance,
                'compliance_details': compliance_details,
                'total_files_checked': len([f for f in test_files if (project_root / f).exists()])
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f"TDD compliance validation failed: {str(e)}",
                'error': str(e)
            }
    
    def _generate_validation_summary(self, total_duration: float) -> Dict[str, Any]:
        """Generate a comprehensive validation summary."""
        passed_tests = [r for r in self.results if r.status == ValidationStatus.PASSED]
        failed_tests = [r for r in self.results if r.status == ValidationStatus.FAILED]
        error_tests = [r for r in self.results if r.status == ValidationStatus.ERROR]
        skipped_tests = [r for r in self.results if r.status == ValidationStatus.SKIPPED]
        
        total_tests = len(self.results)
        success_rate = (len(passed_tests) / total_tests) * 100 if total_tests > 0 else 0
        
        # Determine overall framework readiness
        framework_ready = success_rate >= 90 and len(error_tests) == 0
        
        summary = {
            'validation_timestamp': time.time(),
            'total_duration': total_duration,
            'framework_ready': framework_ready,
            'overall_success_rate': success_rate,
            'test_summary': {
                'total_tests': total_tests,
                'passed': len(passed_tests),
                'failed': len(failed_tests),
                'errors': len(error_tests),
                'skipped': len(skipped_tests)
            },
            'detailed_results': [asdict(result) for result in self.results],
            'recommendations': self._generate_recommendations()
        }
        
        return summary
    
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on validation results."""
        recommendations = []
        
        failed_tests = [r for r in self.results if r.status == ValidationStatus.FAILED]
        error_tests = [r for r in self.results if r.status == ValidationStatus.ERROR]
        
        if failed_tests:
            recommendations.append(f"Address {len(failed_tests)} failed validation tests")
            
        if error_tests:
            recommendations.append(f"Fix {len(error_tests)} validation tests with errors")
        
        # Specific recommendations based on test names
        for result in failed_tests + error_tests:
            if 'file_structure' in result.test_name:
                recommendations.append("Ensure all required test files are present and syntactically correct")
            elif 'fixtures' in result.test_name:
                recommendations.append("Verify test fixtures are properly defined in conftest.py")
            elif 'mock_components' in result.test_name:
                recommendations.append("Implement missing mock components for testing")
            elif 'performance' in result.test_name:
                recommendations.append("Set up performance monitoring infrastructure")
            elif 'error_injection' in result.test_name:
                recommendations.append("Implement error injection mechanisms for resilience testing")
            elif 'safety' in result.test_name:
                recommendations.append("Complete safety constraint testing implementation")
            elif 'deployment' in result.test_name:
                recommendations.append("Implement deployment configuration testing")
            elif 'tdd_compliance' in result.test_name:
                recommendations.append("Improve test structure to follow TDD principles")
        
        if not recommendations:
            recommendations.append("Test framework validation passed - ready for integration testing")
        
        return list(set(recommendations))  # Remove duplicates
    
    def print_validation_report(self, summary: Dict[str, Any]) -> None:
        """Print a formatted validation report to console."""
        print("\n" + "="*80)
        print("SAFLA INTEGRATION TEST FRAMEWORK VALIDATION REPORT")
        print("="*80)
        
        print(f"Validation Duration: {summary['total_duration']:.2f} seconds")
        print(f"Framework Ready: {'‚úÖ YES' if summary['framework_ready'] else '‚ùå NO'}")
        print(f"Overall Success Rate: {summary['overall_success_rate']:.1f}%")
        
        print("\nTEST SUMMARY")
        print("-" * 40)
        test_summary = summary['test_summary']
        print(f"Total Tests: {test_summary['total_tests']}")
        print(f"Passed: {test_summary['passed']}")
        print(f"Failed: {test_summary['failed']}")
        print(f"Errors: {test_summary['errors']}")
        print(f"Skipped: {test_summary['skipped']}")
        
        print("\nDETAILED RESULTS")
        print("-" * 40)
        for result in self.results:
            status_icon = {
                ValidationStatus.PASSED: "‚úÖ",
                ValidationStatus.FAILED: "‚ùå", 
                ValidationStatus.ERROR: "üí•",
                ValidationStatus.SKIPPED: "‚è≠Ô∏è"
            }[result.status]
            
            print(f"{status_icon} {result.test_name} ({result.duration:.2f}s)")
            if result.status != ValidationStatus.PASSED:
                print(f"   {result.message}")
        
        print("\nRECOMMENDATIONS")
        print("-" * 40)
        for i, recommendation in enumerate(summary['recommendations'], 1):
            print(f"{i}. {recommendation}")
        
        print("\n" + "="*80)

async def main():
    """Main validation execution function."""
    validator = TestFrameworkValidator()
    
    try:
        # Run validation
        summary = await validator.run_validation()
        
        # Print console report
        validator.print_validation_report(summary)
        
        # Save JSON report
        report_path = project_root / "tests" / "integration" / "framework_validation_report.json"
        with open(report_path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"\nDetailed validation report saved to: {report_path}")
        
        # Exit with appropriate code
        if summary['framework_ready']:
            print("\nüéâ Test framework validation PASSED - Ready for integration testing!")
            sys.exit(0)
        else:
            print("\n‚ö†Ô∏è  Test framework validation FAILED - Address issues before proceeding")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nüí• Validation failed with error: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())