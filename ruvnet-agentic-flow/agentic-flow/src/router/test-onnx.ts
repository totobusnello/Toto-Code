#!/usr/bin/env node
/**
 * Test script for ONNX Runtime integration
 * Tests CPU inference with Phi-3 model
 */

import { ONNXProvider } from './providers/onnx.js';

async function testONNXProvider() {
  console.log('üß™ Testing ONNX Runtime Provider\n');

  try {
    // Test 1: Initialize ONNX provider
    console.log('Test 1: Provider Initialization');
    console.log('================================');

    const provider = new ONNXProvider({
      modelId: 'Xenova/Phi-3-mini-4k-instruct',
      maxTokens: 100,
      temperature: 0.7
    });

    console.log(`‚úÖ Provider initialized: ${provider.name}`);
    console.log(`üìä Supports streaming: ${provider.supportsStreaming}`);
    console.log(`üîß Supports tools: ${provider.supportsTools}\n`);

    // Test 2: Simple chat completion
    console.log('Test 2: Chat Completion (CPU)');
    console.log('==============================');

    const chatParams = {
      model: 'Xenova/Phi-3-mini-4k-instruct',
      messages: [
        {
          role: 'user' as const,
          content: 'Say "Hello from ONNX Runtime!" and nothing else.'
        }
      ],
      maxTokens: 50,
      temperature: 0.5
    };

    console.log(`üì§ Sending request...`);
    console.log(`üìù Prompt: ${chatParams.messages[0].content}\n`);

    const startTime = Date.now();
    const response = await provider.chat(chatParams);
    const latency = Date.now() - startTime;

    console.log('üì• Response received:');
    console.log(`  Provider: ${response.metadata?.provider}`);
    console.log(`  Model: ${response.model}`);
    console.log(`  Latency: ${latency}ms`);
    console.log(`  Stop Reason: ${response.stopReason}`);
    console.log(`  Usage: ${response.usage?.inputTokens} in / ${response.usage?.outputTokens} out`);
    console.log(`  Cost: $${response.metadata?.cost?.toFixed(6) || 0} (FREE - Local inference)`);
    console.log(`  Execution Providers: ${response.metadata?.executionProviders?.join(', ')}`);
    console.log(`\n  Content:`);

    for (const block of response.content) {
      if (block.type === 'text') {
        console.log(`    ${block.text}`);
      }
    }

    console.log('\n‚úÖ Test 2 passed!\n');

    // Test 3: Multi-turn conversation
    console.log('Test 3: Multi-Turn Conversation');
    console.log('================================');

    const conversationParams = {
      model: 'Xenova/Phi-3-mini-4k-instruct',
      messages: [
        {
          role: 'user' as const,
          content: 'What is 2+2?'
        },
        {
          role: 'assistant' as const,
          content: '4'
        },
        {
          role: 'user' as const,
          content: 'What about 2+3?'
        }
      ],
      maxTokens: 50,
      temperature: 0.3
    };

    console.log(`üì§ Multi-turn conversation...`);
    const convResponse = await provider.chat(conversationParams);

    console.log('üì• Response:');
    console.log(`  Content: ${convResponse.content[0].type === 'text' ? convResponse.content[0].text : 'N/A'}`);
    console.log('\n‚úÖ Test 3 passed!\n');

    // Test 4: Model info
    console.log('Test 4: Model Information');
    console.log('=========================');

    const modelInfo = provider.getModelInfo();
    console.log(`üìä Model ID: ${modelInfo.modelId}`);
    console.log(`üîß Execution Providers: ${modelInfo.executionProviders.join(', ')}`);
    console.log(`‚ö° GPU Support: ${modelInfo.supportsGPU ? 'Yes' : 'No (CPU only)'}`);
    console.log(`‚úì Initialized: ${modelInfo.initialized}`);
    console.log('\n‚úÖ Test 4 passed!\n');

    // Test 5: Performance benchmark
    console.log('Test 5: Performance Benchmark');
    console.log('=============================');

    const benchmarkParams = {
      model: 'Xenova/Phi-3-mini-4k-instruct',
      messages: [
        {
          role: 'user' as const,
          content: 'Count from 1 to 5.'
        }
      ],
      maxTokens: 50,
      temperature: 0.5
    };

    const benchmarkRuns = 3;
    const latencies: number[] = [];

    for (let i = 0; i < benchmarkRuns; i++) {
      const start = Date.now();
      await provider.chat(benchmarkParams);
      const duration = Date.now() - start;
      latencies.push(duration);
      console.log(`  Run ${i + 1}: ${duration}ms`);
    }

    const avgLatency = latencies.reduce((a, b) => a + b, 0) / latencies.length;
    const tokensPerSec = (50 / avgLatency) * 1000;

    console.log(`\nüìä Benchmark Results:`);
    console.log(`  Average Latency: ${avgLatency.toFixed(0)}ms`);
    console.log(`  Tokens/Second: ${tokensPerSec.toFixed(1)}`);
    console.log('\n‚úÖ Test 5 passed!\n');

    // Cleanup
    await provider.dispose();

    // Final summary
    console.log('üéâ All ONNX Tests Passed!');
    console.log('=========================');
    console.log(`‚úÖ Provider initialization working`);
    console.log(`‚úÖ CPU inference functional`);
    console.log(`‚úÖ Chat completion successful`);
    console.log(`‚úÖ Multi-turn conversations working`);
    console.log(`‚úÖ Performance: ${tokensPerSec.toFixed(1)} tokens/sec`);
    console.log(`‚úÖ Cost: $0.00 (100% free local inference)`);
    console.log(`\nüí° Next Steps:`);
    console.log(`  1. Integrate ONNX provider into router`);
    console.log(`  2. Add GPU support (CUDA/DirectML)`);
    console.log(`  3. Implement model caching`);
    console.log(`  4. Add streaming support`);

  } catch (error) {
    console.error('\n‚ùå Test Failed!');
    console.error('===============');
    console.error(error);
    process.exit(1);
  }
}

// Run tests
testONNXProvider().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});
