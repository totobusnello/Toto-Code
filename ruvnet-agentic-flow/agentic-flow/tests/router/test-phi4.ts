#!/usr/bin/env node
/**
 * Test script for Phi-4 ONNX provider with HuggingFace API fallback
 */

import { ONNXPhi4Provider } from './providers/onnx-phi4.js';

async function testPhi4Provider() {
  console.log('üß™ Testing Phi-4 ONNX Provider (via HuggingFace API)\n');

  try {
    // Test 1: Initialize provider
    console.log('Test 1: Provider Initialization');
    console.log('================================');

    const provider = new ONNXPhi4Provider({
      modelId: 'microsoft/Phi-3-mini-4k-instruct',
      huggingfaceApiKey: process.env.HUGGINGFACE_API_KEY
    });

    const info = provider.getModelInfo();
    console.log(`‚úÖ Provider initialized`);
    console.log(`üìä Model: ${info.modelId}`);
    console.log(`üîß Mode: ${info.mode}`);
    console.log(`‚ö° Supports local: ${info.supportsLocalInference}\n`);

    // Test 2: Simple chat completion
    console.log('Test 2: Chat Completion');
    console.log('=======================');

    const chatParams = {
      model: 'microsoft/Phi-3-mini-4k-instruct',
      messages: [
        {
          role: 'user' as const,
          content: 'What is 2+2? Answer in one sentence.'
        }
      ],
      maxTokens: 50,
      temperature: 0.3
    };

    console.log(`üì§ Sending request...`);
    console.log(`üìù Question: ${chatParams.messages[0].content}\n`);

    const startTime = Date.now();
    const response = await provider.chat(chatParams);
    const latency = Date.now() - startTime;

    console.log('üì• Response received:');
    console.log(`  Provider: ${response.metadata?.provider}`);
    console.log(`  Model: ${response.model}`);
    console.log(`  Mode: ${response.metadata?.mode}`);
    console.log(`  Latency: ${latency}ms`);
    console.log(`  Usage: ${response.usage?.inputTokens} in / ${response.usage?.outputTokens} out`);
    console.log(`  Cost: $${response.metadata?.cost?.toFixed(6) || 0}`);
    console.log(`\n  Content:`);

    for (const block of response.content) {
      if (block.type === 'text') {
        console.log(`    ${block.text}`);
      }
    }

    console.log('\n‚úÖ Test 2 passed!\n');

    // Test 3: Reasoning test
    console.log('Test 3: Reasoning Test');
    console.log('======================');

    const reasoningParams = {
      model: 'microsoft/Phi-3-mini-4k-instruct',
      messages: [
        {
          role: 'user' as const,
          content: 'If a train travels at 60mph for 2 hours, how far does it go?'
        }
      ],
      maxTokens: 100,
      temperature: 0.5
    };

    console.log(`üì§ Testing reasoning...`);
    const reasoningResponse = await provider.chat(reasoningParams);

    console.log('üì• Response:');
    console.log(`  ${reasoningResponse.content[0].type === 'text' ? reasoningResponse.content[0].text : 'N/A'}`);
    console.log('\n‚úÖ Test 3 passed!\n');

    // Test 4: Model info
    console.log('Test 4: Model Information');
    console.log('=========================');

    const modelInfo = provider.getModelInfo();
    console.log(`üìä Model ID: ${modelInfo.modelId}`);
    console.log(`üîß Inference Mode: ${modelInfo.mode}`);
    console.log(`üìÅ Model Path: ${modelInfo.modelPath}`);
    console.log(`üîë API Key: ${modelInfo.apiKey || 'Not set'}`);
    console.log(`‚ö° Local ONNX Ready: ${modelInfo.supportsLocalInference}`);
    console.log('\n‚úÖ Test 4 passed!\n');

    // Test 5: Performance benchmark
    console.log('Test 5: Performance Benchmark');
    console.log('=============================');

    const benchmarkParams = {
      model: 'microsoft/Phi-3-mini-4k-instruct',
      messages: [
        {
          role: 'user' as const,
          content: 'Count from 1 to 5.'
        }
      ],
      maxTokens: 30,
      temperature: 0.5
    };

    const benchmarkRuns = 3;
    const latencies: number[] = [];

    for (let i = 0; i < benchmarkRuns; i++) {
      const start = Date.now();
      await provider.chat(benchmarkParams);
      const duration = Date.now() - start;
      latencies.push(duration);
      console.log(`  Run ${i + 1}: ${duration}ms`);
    }

    const avgLatency = latencies.reduce((a, b) => a + b, 0) / latencies.length;

    console.log(`\nüìä Benchmark Results:`);
    console.log(`  Average Latency: ${avgLatency.toFixed(0)}ms`);
    console.log(`  Min Latency: ${Math.min(...latencies)}ms`);
    console.log(`  Max Latency: ${Math.max(...latencies)}ms`);
    console.log('\n‚úÖ Test 5 passed!\n');

    // Final summary
    console.log('üéâ All Phi-4 Tests Passed!');
    console.log('==========================');
    console.log(`‚úÖ Provider initialization working`);
    console.log(`‚úÖ HuggingFace API inference functional`);
    console.log(`‚úÖ Chat completion successful`);
    console.log(`‚úÖ Reasoning capability verified`);
    console.log(`‚úÖ Performance: ${avgLatency.toFixed(0)}ms average`);
    console.log(`\nüí° Next Steps:`);
    console.log(`  1. Free up disk space (need 5GB)`);
    console.log(`  2. Download model.onnx.data`);
    console.log(`  3. Switch to local ONNX inference`);
    console.log(`  4. Benchmark local vs API performance`);

  } catch (error) {
    console.error('\n‚ùå Test Failed!');
    console.error('===============');
    console.error(error);
    process.exit(1);
  }
}

// Run tests
testPhi4Provider().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});
